global_pattern_1: >
  python experiments/tune_with_scheduler.py
  --env_type <ENV_TYPE>
  --tune <TUNE_PARAM> --num_samples 1
  --no_vf_share_layers
  <OTHER_FLAGS:>
  --tag:core --tag:pbt  --tag:pbt-grouped --tag:pbt-fine --tag:no-shared-encoder
  --buffer_length auto
  --comet offline --wandb offline+upload@end --log_stats learners
  --hostname_selector '!in(dws-login-01,dws-login-02)'
  --total_steps 1048576 --use_exact_total_steps
  pbt --grouped --group_size 3 --perturbation_interval 0.03125 --quantile_fraction
  0.01
# NOTE: Use set +H when using hostname selector
# TODO: lr as > 10 options and so quantile_fraction = 0.1 -> 2 top groups
submit_pattern: >
  ray job submit --working-dir "." --entrypoint-memory 3000000000  --entrypoint-num-cpus
  0.66 --entrypoint-resources '{"persistent_node":1}'
  --submission-id pbt_fine_batch_size_mlp-xxxxxxxxx_restore -- \  python  experiments/tune_with_scheduler.py
  --restore_path yyyyy  --hostname_selector '!in(dws-login-01,dws-login-02)'

  --submission-id pbt_fine_batch_size_InvertedDoublePendulum-v5_7435_restore -- python  experiments/tune_with_scheduler.py
  --restore_path /home/dsperber/master/repos/ray_utilities/outputs/shared/experiments/Default-mlp-InvertedPendulum-v5/pbt-tune_tune:batch_size/Default-mlp-InvertedPendulum-v5-tws47f25112200240e674
  --hostname_selector '!in(dws-login-01,dws-login-02)'
notes: |
  - All here do NOT use KL loss
  - use entropy_coeff=0.01 (default 0.0)
  - Use GAE and critic
# --- Envs --- #
<mujoco_envs>:
- use another key
<mujoco_envs_ext>:
- use another key
<basic_envs>:
- Breakout
- SpaceInvarders
# ---
test_command:
  test:id:
    entrypoint: >
      python
      -c "print('Hello, World!')"
# ---
pbt_fine_batch_size_base:
  entrypoint_pattern:
    pattern: global_pattern_1
    substitutions:
      <TUNE_PARAM>: batch_size
      <ENV_TYPE>: <basic_envs>
  entrypoint_resources:
    # custom resource to not schedule on SLURM nodes that time out too early
    persistent_node: 1
pbt_fine_batch_size_base_large:
  entrypoint_pattern:
    pattern: global_pattern_1
    substitutions:
      <TUNE_PARAM>: batch_size
      # Test first
      <ENV_TYPE>: <basic_envs>
      <OTHER_FLAGS>: |
        --minibatch_size 1024
  entrypoint_resources:
    # custom resource to not schedule on SLURM nodes that time out too early
    persistent_node: 1
pbt_fine_minibatch_size_base:
  comment: tune minibatch_size with fixed train_batch_size_per_learner=2048
  entrypoint_resources:
    persistent_node: 1
  entrypoint_pattern:
    pattern: global_pattern_1
    substitutions:
      <TUNE_PARAM>: minibatch_size
      <ENV_TYPE>: <basic_envs>
      <OTHER_FLAGS>: |-
        --train_batch_size_per_learner 2048
pbt_fine_minibatch_size_large_base:
  comment: tune minibatch_size with fixed train_batch_size_per_learner=8192
  entrypoint_resources:
    persistent_node: 1
  entrypoint_pattern:
    pattern: global_pattern_1
    substitutions:
      <TUNE_PARAM>: minibatch_size
      <ENV_TYPE>: <basic_envs>
      <OTHER_FLAGS>: |-
        --train_batch_size_per_learner 8192
pbt_fine_accumulate:
  entrypoint_pattern:
    pattern: global_pattern_1
    substitutions:
      <TUNE_PARAM>: accumulate_gradients_every
      <ENV_TYPE>: <basic_envs>
  entrypoint_resources:
    persistent_node: 1
pbt_fine_num_envs:
  # This is a comment
  comment: small minibatch_size=256 and train_batch_size=2048
  entrypoint_pattern:
    pattern: global_pattern_1
    substitutions:
      <TUNE_PARAM>: num_envs_per_env_runner
      <ENV_TYPE>: <basic_envs>
  entrypoint_resources:
    persistent_node: 1
pbt_fine_num_envs_large:
  comment: minibatch_size=516 and train_batch_size_per_learner=8192 (same 1/16)
    scale
  entrypoint_resources:
    persistent_node: 1
  entrypoint_pattern:
    pattern: global_pattern_1
    substitutions:
      <TUNE_PARAM>: num_envs_per_env_runner
      <ENV_TYPE>: <basic_envs>
      <OTHER_FLAGS>: |
        --minibatch_size 1024
        --train_batch_size_per_learner 8192

pbt_fine_clip_param:
  # TODO: Change to base
  <ENV_TYPE>:
  - CartPole-v1
  - Acrobot-v1
  - LunarLander-v3
  entrypoint_resources:
    persistent_node: 1
  entrypoint_pattern:
    pattern: global_pattern_1
    substitutions:
      <TUNE_PARAM>: clip_param
      <ENV_TYPE>: <basic_envs>
pbt_fine_clip_param_base_large:
  # Change to non-base
  entrypoint_resources:
    persistent_node: 1
  entrypoint_pattern:
    pattern: global_pattern_1
    substitutions:
      <TUNE_PARAM>: clip_param
      <ENV_TYPE>: <basic_envs>
      <OTHER_FLAGS>: |
        --minibatch_size 1024
        --train_batch_size_per_learner 8192
pbt_fine_entropy_coeff_base:
  # TODO: Change to base
  <ENV_TYPE>:
  - CartPole-v1
  - Acrobot-v1
  - LunarLander-v3
  entrypoint_resources:
    persistent_node: 1
  entrypoint_pattern:
    pattern: global_pattern_1
    substitutions:
      <TUNE_PARAM>: entropy_coeff
      <ENV_TYPE>: <basic_envs>
pbt_fine_vf_loss_coeff_base:
  # TODO: Change to base
  <ENV_TYPE>:
  - CartPole-v1
  - Acrobot-v1
  - LunarLander-v3
  entrypoint_resources:
    persistent_node: 1
  entrypoint_pattern:
    pattern: global_pattern_1
    substitutions:
      <TUNE_PARAM>: vf_loss_coeff
      <ENV_TYPE>: <basic_envs>
pbt_fine_gamma_base:
  entrypoint_pattern:
    pattern: global_pattern_1
    substitutions:
      <TUNE_PARAM>: gamma
      <ENV_TYPE>: <basic_envs>
  entrypoint_resources:
    persistent_node: 1

pbt_fine_gamma_large:
  entrypoint_pattern:
    pattern: global_pattern_1
    substitutions:
      <TUNE_PARAM>: gamma
      <ENV_TYPE>: <basic_envs>
      <OTHER_FLAGS>: |
        --minibatch_size 1024
        --train_batch_size_per_learner 8192
  entrypoint_resources:
    persistent_node: 1
pbt_fine_lr_base:
  entrypoint_resources:
    persistent_node: 1
  entrypoint_pattern:
    pattern: global_pattern_1
    substitutions:
      <TUNE_PARAM>: lr
      <ENV_TYPE>: <basic_envs>
pbt_fine_lr_large:
  comment: minibatch_size=516 and train_batch_size_per_learner=8192 (same 1/16)
    scale  - need to repeat because of 2 top groups
  entrypoint_resources:
    persistent_node: 1
  entrypoint_pattern:
    pattern: global_pattern_1
    substitutions:
      <TUNE_PARAM>: lr
      <ENV_TYPE>: <basic_envs>
      <OTHER_FLAGS>: |
        --minibatch_size 1024
        --train_batch_size_per_learner 8192
pbt_fine_vf_clip_param_base:
  entrypoint_resources:
    persistent_node: 1
  entrypoint_pattern:
    pattern: global_pattern_1
    substitutions:
      <TUNE_PARAM>: vf_clip_param
      <ENV_TYPE>: <basic_envs>

# --- Added gamma and grad_clip groups ---

# region grad clip
pbt_fine_grad_clip_base:
  entrypoint_pattern:
    pattern: global_pattern_1
    substitutions:
      <TUNE_PARAM>: grad_clip
      <ENV_TYPE>: <basic_envs>
  entrypoint_resources:
    persistent_node: 1

pbt_fine_grad_clip_base_large:
  entrypoint_pattern:
    pattern: global_pattern_1
    substitutions:
      <TUNE_PARAM>: grad_clip
      <ENV_TYPE>: <basic_envs>
      <OTHER_FLAGS>: |
        --minibatch_size 1024
        --train_batch_size_per_learner 8192
  entrypoint_resources:
    persistent_node: 1
